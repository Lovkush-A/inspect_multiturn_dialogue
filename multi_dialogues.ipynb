{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals for multi-turn dialogue using Inspect\n",
    "\n",
    "This notebook illustrates how to use [AISI's Inspect open source framework](https://ukgovernmentbeis.github.io/inspect_ai/) to evaluate LLM's in multi-turn dialogues.\n",
    "\n",
    "I use example evals from MT Bench, copied from the [fastchat github repo](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/mt_bench/question.jsonl) and stored in `question.jsonl`.\n",
    "\n",
    "I use models from OpenAI.\n",
    "It should be easy to adjust this example for other models.\n",
    "\n",
    "Contents\n",
    "- How to run the notebook\n",
    "- Create Inspect `Dataset`\n",
    "- Create `solver`\n",
    "- Create `scorer`\n",
    "- Run the evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the notebook\n",
    "\n",
    "1. Locally clone this repository.\n",
    "1. Create a virtual environment with python and install the relevant packages by running:\n",
    "`pip install inspect-ai openai ipykernel ipywidgets\n",
    "`\n",
    "1. Create a `.env` file in the root of this repository with one line:\n",
    "`\n",
    "OPENAI_API_KEY=your-api-key\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from inspect_ai.model import ChatMessageUser, ChatMessageSystem, Model, get_model, ChatMessage, ChatMessageAssistant\n",
    "from inspect_ai.dataset import Sample, json_dataset\n",
    "from inspect_ai.solver import Generate, Solver, TaskState, solver\n",
    "from inspect_ai.scorer import Scorer, Score, scorer, INCORRECT, Target, mean, accuracy, bootstrap_std\n",
    "from inspect_ai import Task, eval, task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inspect `Dataset`\n",
    "\n",
    "Essentially, a `Dataset` is a collection of `Sample` objects.\n",
    "For multi-turn dialogues, a `Sample` consists of a list of user questions that will be asked one after the other in a single dialogue.\n",
    "See a couple of examples below.\n",
    "\n",
    "Note that MT Bench only has two questions per sample, but in principle you can have as many as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 'record' is a dictionary corresponding to one line from the jsonl file.\n",
    "# In particular, it has a key 'turns' that is a list of strings.\n",
    "def record_to_sample(record):\n",
    "    return Sample(\n",
    "        input = [ChatMessageUser(content=turn) for turn in record['turns']],\n",
    "        id=record['question_id']\n",
    "    )\n",
    "\n",
    "dataset = json_dataset(\"question.jsonl\", record_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample(input=[ChatMessageUser(content='Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.', source=None, role='user'), ChatMessageUser(content='Rewrite your previous response. Start every sentence with the letter A.', source=None, role='user')], choices=None, target='', id=81, metadata=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for single_input in dataset[0].input:\n",
    "    print(single_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample(input=[ChatMessageUser(content='Suggest five award-winning documentary films with brief background descriptions for aspiring filmmakers to study.', source=None, role='user'), ChatMessageUser(content='With the spirit in the first film, craft a succinct and persuasive pitch for a film about overcoming adversity.', source=None, role='user')], choices=None, target='', id=160, metadata=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `solver`\n",
    "\n",
    "Solvers are essentially the scaffolding around an LLM to process a single Sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@solver\n",
    "def multi_dialogue_solver() -> Solver:\n",
    "    \"\"\"Solver for the multi-dialogue task.\n",
    "\n",
    "    This solver simulates a dialogue between a user and the LLM.\n",
    "\n",
    "    It assumes that `state.messages` is a list of ChatMessageUser objects.\n",
    "    It will write to `state.messages` a list of ChatMessageUser and ChatMessageSystem\n",
    "    objects corresponding to the dialogue between the user and the LLM.\n",
    "    \"\"\"\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # copy the messages from the state\n",
    "        input = state.messages.copy()\n",
    "\n",
    "        # for this solver, we expect input to be a list of ChatMessageUser\n",
    "        if not isinstance(input, list):\n",
    "            raise TypeError(f'Inputs in samples of the dataset should be list of ChatMessageUser. Found {input}')\n",
    "        if not all(isinstance(turn, ChatMessageUser) for turn in input):\n",
    "            raise TypeError(f'Inputs in samples of the dataset should be list of ChatMessageUser. Found {input}')\n",
    "\n",
    "        # generate the output after each turn in the input and write to state.messages\n",
    "        state.messages = []\n",
    "        for turn in input:\n",
    "            state.messages.append(turn)\n",
    "            state = await generate(state)\n",
    "\n",
    "        return state\n",
    "\n",
    "    return solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `scorer`\n",
    "\n",
    "`model_graded_dialogue`. Scorer that evaluates the LLM's output (created from the solver) by using another LLM.\n",
    "Based on the prompt from MT Bench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt based on 'single-v1-multi-turn' prompt from https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/judge_prompts.jsonl\n",
    "# Changed system prompt so it judges all answers to user questions, not just answer to the second question.\n",
    "# Changed prompt template to remove reference to assistant A and to allow for conversations of arbitrary length.\n",
    "\n",
    "SYSTEM_PROMPT = \"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on all the assistant's answers to the user's questions. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\\n\\n\"\n",
    "PROMPT_TEMPLATE_START = \"<|The Start of the Assistant's Conversation with User|>\\n\\n\"\n",
    "PROMPT_TEMPLATE_USER = \"### User:\\n{message}\\n\\n\"\n",
    "PROMPT_TEMPLATE_ASSISTANT = \"### Assistant:\\n{message}\\n\\n\"\n",
    "PROMPT_TEMPLATE_END = \"<|The End of the Assistant's Conversation with User|>\"\n",
    "\n",
    "# scorer adapted from example of model_graded_qa from https://ukgovernmentbeis.github.io/inspect_ai/scorers.html\n",
    "\n",
    "@scorer(metrics=[mean(), bootstrap_std()])\n",
    "def model_graded_dialogue(\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    prompt_template_start: str = PROMPT_TEMPLATE_START,\n",
    "    prompt_template_user: str = PROMPT_TEMPLATE_USER,\n",
    "    prompt_template_assistant: str = PROMPT_TEMPLATE_ASSISTANT,\n",
    "    prompt_template_end: str = PROMPT_TEMPLATE_END,\n",
    "    model: str | Model | None = None,\n",
    ") -> Scorer:\n",
    "    # resolve model\n",
    "    grader_model = get_model(model)\n",
    "\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        # create system message\n",
    "        system_message = ChatMessageSystem(content=system_prompt)\n",
    "\n",
    "        # create user message, by looping through TaskState.messages\n",
    "        user_message = prompt_template_start\n",
    "        for message in state.messages:\n",
    "            if isinstance(message, ChatMessageUser):\n",
    "                user_message += prompt_template_user.format(message=message.content)\n",
    "            elif isinstance(message, ChatMessageAssistant):\n",
    "                user_message += prompt_template_assistant.format(message=message.content)\n",
    "            else:\n",
    "                continue\n",
    "        user_message += prompt_template_end\n",
    "\n",
    "        user_message = ChatMessageUser(content=user_message)\n",
    "\n",
    "        # query the model for the score\n",
    "        result = await grader_model.generate([system_message, user_message])\n",
    "\n",
    "        # extract the grade\n",
    "        grade_pattern = r\"Rating: \\[\\[(\\d+)\\]\\]\"\n",
    "        match = re.search(grade_pattern, result.completion)\n",
    "        if match:\n",
    "            return Score(\n",
    "                value=int(match.group(1)),\n",
    "                explanation=result.completion,\n",
    "            )\n",
    "        else:\n",
    "            return Score(\n",
    "                value=0,\n",
    "                explanation=\"Grade not found in model output: \"\n",
    "                + f\"{result.completion}\",\n",
    "            )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evals\n",
    "\n",
    "After running an eval, a log will be created as a json file in `./logs/`.\n",
    "You can view the (latest) log in a user friendly GUI by running `inspect view` in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def multi_dialogue_task():\n",
    "    return Task(\n",
    "        dataset=dataset,\n",
    "        plan=[\n",
    "          multi_dialogue_solver(),\n",
    "        ],\n",
    "        scorer=model_graded_dialogue(model=\"openai/gpt-3.5-turbo\")\n",
    "    )\n",
    "\n",
    "logs = eval(\n",
    "    tasks=multi_dialogue_task(),\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inspect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
