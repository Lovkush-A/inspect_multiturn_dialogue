{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals for multi-turn dialogue using Inspect\n",
    "\n",
    "The aim of the notebook is illustrate how one can use [AISI's Inspect open source framework](https://ukgovernmentbeis.github.io/inspect_ai/) to evaluate LLM's in multi-turn dialogues.\n",
    "\n",
    "I use example evals from MT Bench, copied from [here](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/mt_bench/question.jsonl) and stored in `question.jsonl`.\n",
    "\n",
    "I use models from OpenAI.\n",
    "It should be easy to adjust the code/instructions as appropriate for other models.\n",
    "\n",
    "Contents\n",
    "- How to run the notebook\n",
    "- Create Inspect `Dataset`\n",
    "- Create `solver`\n",
    "- Create `scorer`\n",
    "- Run the evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the notebook\n",
    "\n",
    "1. Locally clone this repository.\n",
    "1. Create a virtual environment with python and install the relevant packages by running:\n",
    "`pip install inspect-ai openai ipykernel ipywidgets\n",
    "`\n",
    "1. Create a `.env` file in the root of this repository with one line:\n",
    "`\n",
    "OPENAI_API_KEY=your-api-key\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from inspect_ai.model import ChatMessageUser, ChatMessageSystem, Model, get_model, ChatMessage, ChatMessageAssistant\n",
    "from inspect_ai.dataset import Sample, json_dataset\n",
    "from inspect_ai.solver import Generate, Solver, TaskState, solver\n",
    "from inspect_ai.scorer import Scorer, Score, scorer, INCORRECT, Target, mean, accuracy, bootstrap_std\n",
    "from inspect_ai import Task, eval, task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inspect `Dataset`\n",
    "\n",
    "Essentially, a `Dataset` is a collection of `Sample` objects.\n",
    "For the task of multi-turn dialogues, a `Sample` consists of a list of user questions that will be asked one after the other in single dialogue.\n",
    "See a couple of examples below.\n",
    "Note that MT Bench only had two questions per sample, but I have written things so that in principle you can have as many as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample(record):\n",
    "    return Sample(\n",
    "        input = [ChatMessageUser(content=turn) for turn in record['turns']],\n",
    "        id=record['question_id']\n",
    "    )\n",
    "\n",
    "dataset = json_dataset(\"question.jsonl\", record_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `solver`\n",
    "\n",
    "Solvers are essentially the scaffolding around an LLM to process a single Sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@solver\n",
    "def multi_dialogue_solver() -> Solver:\n",
    "    \"\"\"Solver for the multi-dialogue task.\n",
    "\n",
    "    This solver simulates a dialogue between a user and the LLM.\n",
    "    \n",
    "    The user's  messages are taken from the list contained in a single sample,\n",
    "    which is stored in the TaskState's `_input` variable.\n",
    "\n",
    "    WARNING: This solver will undo any previous solvers' adjustments to the message history,\n",
    "    because I have set `state.messages` to be an empty list. I did this because\n",
    "    I don't know what the state.messages will be if `_input` is a list of ChatMessageUser.\n",
    "    \"\"\"\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # get the input from the state.\n",
    "        input = state._input\n",
    "\n",
    "        # for this solver, we expect input to be a list of ChatMessageUser\n",
    "        if not isinstance(input, list):\n",
    "            raise TypeError(f'Inputs in samples of the dataset should be list of ChatMessageUser. Found {input}')\n",
    "        if not all(isinstance(turn, ChatMessageUser) for turn in input):\n",
    "            raise TypeError(f'Inputs in samples of the dataset should be list of ChatMessageUser. Found {input}')\n",
    "\n",
    "        # I dont know if this is necessary, but it means I know what\n",
    "        # state.messages is.\n",
    "        state.messages = []\n",
    "\n",
    "        # generate the output after each turn in the input\n",
    "        for turn in input:\n",
    "            state.messages.append(turn)\n",
    "            state = await generate(state)\n",
    "\n",
    "        return state\n",
    "\n",
    "    return solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `scorer`\n",
    "\n",
    "I actually create two scorers.\n",
    "\n",
    "- `always_false_scorer`. This was created to help me test the solver.\n",
    "I currently dont know how to run a solver without using it in a full task, so it was easiest to create a dummy scorer.\n",
    "- `model_graded_dialogue`. Scorer that evaluates the LLM's output (created from the solver) by using another LLM.\n",
    "The prompt used to do this is from MT Bench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy(), bootstrap_std()])\n",
    "def always_false_scorer() -> Scorer:\n",
    "    # returns a scorer that always returns incorrect.\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        return Score(\n",
    "            value=INCORRECT,\n",
    "            explanation=\"You are always wrong. Mwahaha\",\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt closely based on 'single-v1-multi-turn' prompt from https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/judge_prompts.jsonl\n",
    "# Changed system prompt so it judges all answers to user questions, not just answer to the second one.\n",
    "# Changed prompt template to remove reference to assistant A and to allow for conversations of arbitrary length.\n",
    "\n",
    "SYSTEM_PROMPT = \"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on all the assistant's answers to the user's questions. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\\n\\n\"\n",
    "PROMPT_TEMPLATE_START = \"<|The Start of the Assistant's Conversation with User|>\\n\\n\"\n",
    "PROMPT_TEMPLATE_USER = \"### User:\\n{message}\\n\\n\"\n",
    "PROMPT_TEMPLATE_ASSISTANT = \"### Assistant:\\n{message}\\n\\n\"\n",
    "PROMPT_TEMPLATE_END = \"<|The End of the Assistant's Conversation with User|>\"\n",
    "\n",
    "# scorer adapted from example of model_graded_qa from https://ukgovernmentbeis.github.io/inspect_ai/scorers.html\n",
    "\n",
    "@scorer(metrics=[mean(), bootstrap_std()])\n",
    "def model_graded_dialogue(\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    prompt_template_start: str = PROMPT_TEMPLATE_START,\n",
    "    prompt_template_user: str = PROMPT_TEMPLATE_USER,\n",
    "    prompt_template_assistant: str = PROMPT_TEMPLATE_ASSISTANT,\n",
    "    prompt_template_end: str = PROMPT_TEMPLATE_END,\n",
    "    model: str | Model | None = None,\n",
    ") -> Scorer:\n",
    "    # resolve model\n",
    "    grader_model = get_model(model)\n",
    "\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        # create system message\n",
    "        system_message = ChatMessageSystem(content=system_prompt)\n",
    "\n",
    "        # create user message, by looping through TaskState.messages\n",
    "        user_message = prompt_template_start\n",
    "        for message in state.messages:\n",
    "            if isinstance(message, ChatMessageUser):\n",
    "                user_message += prompt_template_user.format(message=message.content)\n",
    "            elif isinstance(message, ChatMessageAssistant):\n",
    "                user_message += prompt_template_assistant.format(message=message.content)\n",
    "            else:\n",
    "                continue\n",
    "        user_message += prompt_template_end\n",
    "\n",
    "        user_message = ChatMessageUser(content=user_message)\n",
    "\n",
    "        # query the model for the score\n",
    "        result = await grader_model.generate([system_message, user_message])\n",
    "\n",
    "        # extract the grade\n",
    "        grade_pattern = r\"Rating: \\[\\[(\\d+)\\]\\]\"\n",
    "        match = re.search(grade_pattern, result.completion)\n",
    "        if match:\n",
    "            return Score(\n",
    "                value=int(match.group(1)),\n",
    "                explanation=result.completion,\n",
    "            )\n",
    "        else:\n",
    "            return Score(\n",
    "                value=0,\n",
    "                explanation=\"Grade not found in model output: \"\n",
    "                + f\"{result.completion}\",\n",
    "            )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evals\n",
    "\n",
    "After running an eval, a log will be created as a json file in `./logs/`.\n",
    "You can view the (latest) log in a user friendly GUI by running `inspect view` in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the solver using one sample and basic solver\n",
    "@task\n",
    "def multi_dialogue_task():\n",
    "    return Task(\n",
    "        dataset=dataset[31:32],\n",
    "        plan=[\n",
    "          multi_dialogue_solver(),\n",
    "        ],\n",
    "        scorer=always_false_scorer()\n",
    "    )\n",
    "\n",
    "logs = eval(\n",
    "    tasks=multi_dialogue_task(),\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the full workflow\n",
    "@task\n",
    "def multi_dialogue_task():\n",
    "    return Task(\n",
    "        dataset=dataset,\n",
    "        plan=[\n",
    "          multi_dialogue_solver(),\n",
    "        ],\n",
    "        scorer=model_graded_dialogue(model=\"openai/gpt-3.5-turbo\")\n",
    "    )\n",
    "\n",
    "logs = eval(\n",
    "    tasks=multi_dialogue_task(),\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inspect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
